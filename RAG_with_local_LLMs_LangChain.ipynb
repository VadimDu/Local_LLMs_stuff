{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbf6d1c8",
   "metadata": {},
   "source": [
    "# RAG implementation with LangChain framework with local LLMs\n",
    "In this notebook I create Retrieval-Augmented Generation (RAG) system based on my documents (maybe some scientific papers) for by local LLMs implemented with LangChain framework\n",
    "\n",
    "__What is RAG?__\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is an architecture that augments a foundation LLM with external, up-to-date knowledge via a retrieval step before generation. It decouples the LLM's static training data from dynamic, domain-specific information.\n",
    "\n",
    "__Core Components (Local Setup):__\n",
    "\n",
    "* Vector Database: Stores embeddings of external documents (e.g., ChromaDB, FAISS, Weaviate).\n",
    "\n",
    "* Embedding Model: Converts text → dense vectors (e.g., all-MiniLM-L6-v2, BGE-small).\n",
    "\n",
    "* Retriever: Queries the DB for top-k relevant passages via similarity search (e.g., cosine distance).\n",
    "\n",
    "* LLM: Takes the retrieved context + user query → generates a grounded response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bf4159",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b58fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain_community.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains import RetrievalQA\n",
    "import pdfplumber\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611e2c8b",
   "metadata": {},
   "source": [
    "##  Data Ingestion and VectorDB Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80fdb64",
   "metadata": {},
   "source": [
    "__Notes on the different document chunking strategies:__\n",
    "\n",
    "__Why to split the documents?:__\n",
    "It gives the RAG retriever fine‑grained pieces that fit inside the LLM’s context window\n",
    "\n",
    "__SemanticChunker:__\n",
    "\n",
    "Mechanism:\n",
    "This splitter leverages embeddings or language models to understand the semantic relationships between sentences. It identifies \"breakpoints\" where the semantic similarity between consecutive sentences or sentence groups falls below a certain threshold, indicating a natural point to create a new chunk.\n",
    "\n",
    "__RecursiveCharacterTextSplitter:__\n",
    "\n",
    "Mechanism:\n",
    "This splitter operates by recursively splitting text based on a predefined list of separators, such as [\"\\n\\n\", \"\\n\", \" \", \"\"]. It prioritizes larger, more semantically coherent units (like paragraphs) and only breaks them down further if they exceed the specified chunk_size.\n",
    "\n",
    "\n",
    "* Use __SemanticChunker__ for structured docs (where meaning > text position).\n",
    "* Use __RecursiveCharacterTextSplitter__ with overlap=10-20% for unstructured text (e.g., social media, chat logs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "12d48167",
   "metadata": {},
   "outputs": [],
   "source": [
    "workdir = Path.home()/\"Downloads\"/\"RAG_personal_knowledge_base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "191b2b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF has 31 pages\n",
      "First page text: Article\n",
      "Using artificial intelligence to document the hidden\n",
      "RNA virosphere\n",
      "Graphical abstract Autho...\n"
     ]
    }
   ],
   "source": [
    "# Check if pdfplumber can open my PDF files\n",
    "with pdfplumber.open(workdir/\"Using artificial intelligence to document the hidden RNA virosphere.pdf\") as pdf:\n",
    "    print(f\"PDF has {len(pdf.pages)} pages\")\n",
    "    text = pdf.pages[0].extract_text()\n",
    "    print(f\"First page text: {text[:100]}...\")  # Check if text exists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9af07543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 31 pages from the PDF.\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(\n",
    "    file_path=workdir/\"Using artificial intelligence to document the hidden RNA virosphere.pdf\", \n",
    "    extract_images=False,\n",
    ")\n",
    "docs = loader.load()\n",
    "print(f\"Loaded {len(docs)} pages from the PDF.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9718aa39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article\n",
      "Using artiﬁcial intelligence to document the hidden\n",
      "RNA virosphere\n",
      "Graphical abstract\n",
      "Highlights\n",
      "d AI-based metagenomic mining greatly expands the diversity\n",
      "of the global RNA virosphere\n",
      "d Deve\n",
      "Article\n",
      "Using artiﬁcial intelligence to document\n",
      "the hidden RNA virosphere\n",
      "Xin Hou,1,20 Yong He,2,20 Pan Fang,2 Shi-Qiang Mei,1 Zan Xu,2 Wei-Chen Wu,1 Jun-Hua Tian,3 Shun Zhang,2\n",
      "Zhen-Yu Zeng,2 Qin-Yu\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:200])  # Display first 500 characters of the first page\n",
    "print(docs[1].page_content[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b107a5ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'Acrobat Distiller 8.1.0 (Windows)',\n",
       " 'creator': 'Elsevier',\n",
       " 'creationdate': '2024-10-04T01:13:54+05:30',\n",
       " 'subject': 'Cell, Corrected proof. doi:10.1016/j.cell.2024.09.027',\n",
       " 'author': 'Xin Hou',\n",
       " 'grabs': 'true',\n",
       " 'elsevierwebpdfspecifications': '7.0',\n",
       " 'robots': 'noindex',\n",
       " 'moddate': '2024-10-04T01:17:57+05:30',\n",
       " 'doi': '10.1016/j.cell.2024.09.027',\n",
       " 'title': 'Using artificial intelligence to document the hidden RNA virosphere',\n",
       " 'source': '/Users/danid/Downloads/RAG_docs_v1/Using artificial intelligence to document the hidden RNA virosphere.pdf',\n",
       " 'total_pages': 31,\n",
       " 'page': 10,\n",
       " 'page_label': '11'}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "09036100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. DATA INGESTION (Handles PDFs) ===\n",
    "\n",
    "def load_pdfs_from_dir(pdf_dir: str) -> list:\n",
    "    \"\"\"Scan directory, load PDFs with metadata, skip bad files\"\"\"\n",
    "    all_docs = []\n",
    "    pdf_files = list(Path(pdf_dir).glob(\"*.pdf\"))\n",
    "    \n",
    "    for pdf_path in pdf_files:\n",
    "        try:\n",
    "            loader = PyPDFLoader(\n",
    "                file_path=str(pdf_path),\n",
    "                extract_images=False,\n",
    "            )\n",
    "            docs = loader.load()\n",
    "            print(f\"Loaded {len(docs)} pages from {pdf_path.name}\")\n",
    "            \n",
    "            # Attach metadata (source path)\n",
    "            for doc in docs:\n",
    "                doc.metadata.update({\n",
    "                    \"source\": str(pdf_path.name),\n",
    "                })\n",
    "            all_docs.extend(docs)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Skipped {pdf_path.name}: {str(e)}\")\n",
    "    print(f\"Total documents pages loaded: {len(all_docs)}\")\n",
    "\n",
    "    return all_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "1a0e32ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 pages from Diffusion Sequence Models for Enhanced Protein Representation and Generation.pdf\n",
      "Loaded 22 pages from SaProt- Protein Language Modeling with Structure-aware Vocabulary.pdf\n",
      "Loaded 16 pages from ProtTrans - Toward Understanding the Language of Life Through Self-Supervised Learning.pdf\n",
      "Loaded 31 pages from Using artificial intelligence to document the hidden RNA virosphere.pdf\n",
      "Total documents pages loaded: 89\n"
     ]
    }
   ],
   "source": [
    "documents = load_pdfs_from_dir(workdir/'docs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "0c3d43fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fig. 3.(Left) Plot of natural vs. generation AV token 1-mers with χ2 values and JS. (Right) Word cloud of natural and generated sequence annotations. Higher frequency terms\n",
      "have a bigger font, and ter\n"
     ]
    }
   ],
   "source": [
    "print(documents[5].page_content[0:200])  # Display first 200 characters of the 6th page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "17043d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2. CHUNK TEXT (OPTIMIZED FOR RAG) ===\n",
    "\n",
    "def chunk_docs(docs: list) -> list:\n",
    "    '''Split documents into smaller chunks for RAG, optimized for LLMs. \n",
    "    chunk_size: Max characters per text segment (e.g., 500). Too small: context loss. Too large: LLM overflows/context noise.\n",
    "    chunk_overlap: Characters reused from previous chunk (e.g., 50). Prevents splitting mid-sentence → preserves context across chunks.\n",
    "    '''\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1200, \n",
    "        chunk_overlap=120, # 10-20% overlap\n",
    "        length_function=len, # Function to measure text length (characters not tokens!)\n",
    "        is_separator_regex=False,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\", \".\"],\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(docs)\n",
    "    print(f\"Total chunks created: {len(split_docs)}\")\n",
    "    return split_docs \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "c5e99262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 366\n"
     ]
    }
   ],
   "source": [
    "split_documents = chunk_docs(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "88742abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "so far5–8,10,35 (Figure 1C). This expansion encompasses both ex-\n",
      "isting viral supergroups as well as the discovery of 60 highly\n",
      "divergent supergroups that have largely been overlooked in pre-\n",
      "vious RNA virus discovery projects (Figure 1D). The virus super-\n",
      "groups identiﬁed here were largely comparable to the existing\n",
      "classiﬁcation system at the phylum (e.g., phylumLenarviricota\n",
      "in the case of the Narna-Levi supergroup) or class (e.g., theStel-\n",
      "paviricetes, Alsuviricetes, and Flasuviricetes classes for the\n",
      "Astro-Poty, Hepe-Virga, and Flavi supergroups) levels, high-\n",
      "lighting the extent of the phylogenetic diversity identiﬁed here.\n",
      "Despite the large expansion in RNA virus diversity documented\n",
      "here, major gaps remain in our understanding of the evolution and\n",
      "ecology of the newly discovered viruses. In particular, the hosts\n",
      "for most of the viruses identiﬁed remain unknown. As the majority\n",
      "of current known RNA viruses infect eukaryotes,\n",
      "46,47 and microbi-\n",
      "al eukaryotes exist in great abundance and diversity in natural en-\n",
      "vironments,48,49 it is possible that the viral clades and super-\n",
      "groups identiﬁed here were largely associated with diverse\n"
     ]
    }
   ],
   "source": [
    "print(split_documents[305].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "b8adc578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creator': 'Elsevier', 'creationdate': '2024-10-04T01:13:54+05:30', 'subject': 'Cell, Corrected proof. doi:10.1016/j.cell.2024.09.027', 'author': 'Xin Hou', 'grabs': 'true', 'elsevierwebpdfspecifications': '7.0', 'robots': 'noindex', 'moddate': '2024-10-04T01:17:57+05:30', 'doi': '10.1016/j.cell.2024.09.027', 'title': 'Using artificial intelligence to document the hidden RNA virosphere', 'source': 'Using artificial intelligence to document the hidden RNA virosphere.pdf', 'total_pages': 31, 'page': 11, 'page_label': '12'}\n"
     ]
    }
   ],
   "source": [
    "print(split_documents[305].metadata)  # Check metadata of a chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b714a4b1",
   "metadata": {},
   "source": [
    "__Note about SemanticChunker mechanism:__\n",
    "* SemanticChunker requires embeddings (you pass it via constructor), but the input is text.\n",
    "* It computes embeddings during splitting to find break points in the text to create chunks.\n",
    "* It returns ONLY text chunks (no embeddings).\n",
    "* You must manage embedding computation to avoid double-work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362d71d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# embdedding and LLM models config\n",
    "EMBED_MODEL_NAME = \"BAAI/bge-small-en-v1.5\"  # Lightweight SOTA embedding model\n",
    "# LLM_MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"  # HuggingFace repo (public)\n",
    "\n",
    "# device setup\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "else:\n",
    "    DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "8a5ab7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2. OPTIMIZED CHUNKING (Critical for technical docs!) ===\n",
    "\n",
    "def chunks_docs_semantic(documents, chunk_size=500):\n",
    "    \"\"\"Semantic + recursive splitting to preserve context (no broken code snippets!).\"\"\"\n",
    "    # Use semantic chunking first, then fallback to fixed size\n",
    "    text_splitter = SemanticChunker(\n",
    "        HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME),\n",
    "        breakpoint_threshold_type=\"percentile\",  # Optimized for docs\n",
    "        breakpoint_threshold_amount=85,  # Threshold for splits\n",
    "        number_of_chunks=chunk_size,\n",
    "    )\n",
    "    \n",
    "    # Fallback to recursive if semantic fails (e.g., for simple text)\n",
    "    # chunks = []\n",
    "    # for doc in documents:\n",
    "    #     chunks.extend(text_splitter.split_documents([doc]))\n",
    "    # print(f\"Total semantic chunks created: {len(chunks)}\")\n",
    "\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Total semantic chunks created: {len(chunks)}\")\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "e3a871c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total semantic chunks created: 3665\n"
     ]
    }
   ],
   "source": [
    "split_documents_semantic = chunks_docs_semantic(documents, chunk_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "2d95a076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result was two corpora of amino acid sequences of exactly the same length, 9,989 total\n",
      "after removing entries less than 20 or greater than 2,048 long.\n",
      "D.2.2 F ORMULA\n",
      "Previous residue-based PLMs like the ESM models predict mutational effects using the log odds\n",
      "ratio at the mutated position.\n"
     ]
    }
   ],
   "source": [
    "print(split_documents_semantic[115].page_content)\n",
    "print(split_documents_semantic[1520].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "a936da27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3. INDEXING (Local ChromaDB + BGE embeddings) ===\n",
    "\n",
    "def build_vector_db(chunks, db_path):\n",
    "    \"\"\"Build ChromaDB vector store with BGE-small embeddings.\"\"\"\n",
    "    os.makedirs(db_path, exist_ok=True)\n",
    "\n",
    "    # Initialize embedding model (runs locally)\n",
    "    embeddings = HuggingFaceEmbeddings(  # HuggingFace sentence_transformers embedding models\n",
    "        model_name=EMBED_MODEL_NAME,\n",
    "        model_kwargs={'device': DEVICE},\n",
    "        encode_kwargs={\n",
    "        'normalize_embeddings': True,  # Crucial for cosine similarity\n",
    "        'batch_size': 32,  # Batch processing speedup\n",
    "        'dtype': torch.float  # (fp32), can also use: torch.bfloat16,\n",
    "    })\n",
    "\n",
    "    # Create ChromaDB vector store (persist = disk-backed)\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=str(db_path), # persist to disk\n",
    "        collection_name=\"personal_knowledge\"\n",
    "    )\n",
    "    print(f\"🗄️ Vector store persisted at {str(db_path)}\")\n",
    "\n",
    "    return vectordb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "85c177df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗄️ Vector store persisted at /Users/danid/Downloads/RAG_personal_knowledge_base/chroma_db\n"
     ]
    }
   ],
   "source": [
    "my_vectordb_sem = build_vector_db(chunks=split_documents_semantic, db_path=workdir/\"chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "8cb2e281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗄️ Vector store persisted at /Users/danid/Downloads/RAG_personal_knowledge_base/chroma_db_v2\n"
     ]
    }
   ],
   "source": [
    "my_vectordb = build_vector_db(chunks=split_documents, db_path=workdir/\"chroma_db_v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4cd95f",
   "metadata": {},
   "source": [
    "## Retrieval + Generation\n",
    "This parts includes local LLM setup, RAG chain creation, local data retrieval and prompt generation.\n",
    "\n",
    "__Local Model Loading Options:__\n",
    "* Option 1: Direct Transformers Loading (`from transformers import AutoTokenizer, AutoModelForCausalLM`)\n",
    "* Option 2: LangChain Wrapper (`from langchain.llms.huggingface_pipeline import HuggingFacePipeline`)\n",
    "* Option 3: LM-Studio API (`langchain_community.chat_models import ChatOpenAI`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "7bdf9de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LOCAL LLM SETUP (LM-Studio API) ===\n",
    "\n",
    "# hf_api_token = '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'\n",
    "# os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = hf_api_token\n",
    "my_llms = {'qwen3-coder': 'qwen3-coder-30b-a3b-instruct', \n",
    "           'qwen3-thinking': 'qwen3-30b-a3b-thinking-2507',\n",
    "           'glm-4.5-air': 'glm-4.5-air-mlx',\n",
    "           'gpt-oss-lm': 'lmstudio-community/gpt-oss-120b',\n",
    "           'gpt-oss-us': 'unsloth/gpt-oss-120b'\n",
    "           }\n",
    "BASE_URL = \"http://localhost:1234/v1\"  # LM-Studio local API\n",
    "API_KEY = \"not-needed\"\n",
    "\n",
    "def local_llm_setup(model_name: str, base_url: str, api_key: str):\n",
    "    \"\"\"Initialize local LLM via LM-Studio's OpenAI-compatible endpoint.\"\"\"\n",
    "\n",
    "    llm = ChatOpenAI(\n",
    "        model_name=model_name,\n",
    "        openai_api_key=API_KEY,\n",
    "        openai_api_base=BASE_URL,\n",
    "        temperature=0.7,\n",
    "        max_tokens=8000,\n",
    "    )\n",
    "\n",
    "    return llm\n",
    "\n",
    "\n",
    "my_llm = local_llm_setup(model_name=my_llms['glm-4.5-air'], base_url=BASE_URL, api_key=API_KEY)\n",
    "# check if the model is loaded correctly and responsive\n",
    "# response = my_llm.invoke(\"Hello model\")\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "ad070399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === RAG Chain Creation - Build retrieval-augmented QA chain ===\n",
    "# Builds a RAG chain that:\n",
    "# Retrieves relevant documents using retriever (embedding-based)\n",
    "# Formatted documents with context passed them to the LLM for answer generation \n",
    "# e.g: Prompt: \"Context: [doc1]\\n\\n[doc2]\\n\\n...\\n\\nQuestion: {user_question} → Answer?\"\n",
    "\n",
    "def build_qa_chain(llm, vectordb):\n",
    "    \"\"\"Builds a RetrievalQA chain with the given LLM and vector store.\n",
    "        can configure the retriever parameters in search_kwargs.\"\"\"\n",
    "\n",
    "    retriever = vectordb.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            # search_type=\"similarity_score_threshold\", # by default, the vector store retriever uses similarity search\n",
    "            search_kwargs={\n",
    "                'k': 50,          # top-k most relevant docs [*chunks*] retrieval\n",
    "                # 'fetch_k': 10,    # Internal candidate pool, top-fetch_k fetched\n",
    "                # 'filter': {'source': 'research_papers'},  # Filter by metadata\n",
    "                # \"score_threshold\": 0.5,  # min similarity score for retrieval\n",
    "            }\n",
    "        )\n",
    "\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",  # All context stuffed into prompt\n",
    "        retriever = retriever,\n",
    "        return_source_documents=True,  # For source verification\n",
    "        chain_type_kwargs={\"document_separator\": \"\\n\\n\"  # Clean context formatting, improves prompt readability for the LLM.\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return qa_chain, retriever\n",
    "\n",
    "\n",
    "my_qa_chain, my_retriever = build_qa_chain(llm=my_llm, vectordb=my_vectordb)\n",
    "\n",
    "# === Query Interface ===\n",
    "\n",
    "# Interactive query loop\n",
    "def ask_question(query, qa_chain=my_qa_chain):\n",
    "    result = qa_chain.invoke(query)\n",
    "\n",
    "    print(\"\\n=== ANSWER ===\")\n",
    "    print(result['result'])\n",
    "    \n",
    "    print(\"\\n=== SOURCES ===\")\n",
    "    for doc in result['source_documents']:\n",
    "        print(f\"Content: {doc.page_content[:200]}...\")\n",
    "        print(f\"Metadata: {doc.metadata['title']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeda6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example queries:\n",
    "ask_question('''How does the SaProt protein-LLM differs from other protein-LLMs such as ProtT5, ProtElectra and ESM-2? \n",
    "             What are the key architectural novelties of this model?\n",
    "             Use the given context (source documents) to answer the question.\n",
    "             Give the name of the source documents and metadata that supports your answer.''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "53ea853e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Prompt Injection\n",
    "# Force the chain to use your custom prompt that explicitly mandates context usage:\n",
    "\n",
    "template = \"\"\"Use ONLY the following context (source documents and metadata) to answer the question. \n",
    "Do not use prior knowledge.\n",
    "If the context doesn't contain the answer, respond with \"INSUFFICIENT CONTEXT\".\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=my_llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=my_retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}  # Override default\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "e004aae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_chain.invoke('''How does the SaProt protein-LLM differs from other protein-LLMs such as ProtT5, ProtElectra and ESM-2? \n",
    "                         A newer model called Diffusion Sequence Model (DSM) is suppose to be state-of-the-art. Compare it to SaProt. \n",
    "                        What are the key architectural novelties of this model?\n",
    "                        Use the given context (source documents) to answer the question.\n",
    "                        Give the name of the source documents and metadata that supports your answer.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "c109f1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ANSWER ===\n",
      "\n",
      "Hmm, let me tackle this question step by step. The user wants to know how SaProt differs from ProtT5, ProtElectra, and ESM-2, and also how the newer Diffusion Sequence Model (DSM) compares to SaProt. They specifically asked for key architectural novelties based only on the provided context.\n",
      "\n",
      "Looking through the context, I see multiple documents. There's \"SAPROT: PROTEIN LANGUAGE MODELING WITH STRUCTURE-AWARE VOCABULARY\" which seems to be about SaProt. Then there's \"Diffusion Sequence Models for Enhanced Protein Representation and Generation\" which discusses DSM. The user mentioned ProtT5, ProtElectra and ESM-2, but I need to check if all are covered in the context.\n",
      "\n",
      "For SaProt vs others:\n",
      "- The SaProt document clearly states it uses a \"structure-aware vocabulary\" combining residue and structure tokens derived from Foldseek encoding. This is different from ProtT5 which only does bilingual translation between residue and structure tokens without combining them. \n",
      "- ESM-2 is mentioned as a baseline that lacks explicit structure information, while SaProt explicitly integrates it through its vocabulary.\n",
      "- ProtElectra isn't directly compared in the context, but since SaProt outperforms ESM-2 and other structure-based models like MIF-ST, it's implied SaProt has advantages.\n",
      "\n",
      "For DSM vs SaProt:\n",
      "- The DSM document presents it as a novel approach extending ESM2 with masked diffusion for both generation and representation. Its key novelties include:\n",
      "  - Masked diffusion process enabling sequence generation\n",
      "  - Single-pass reconstruction capability (vs iterative methods)\n",
      "  - High-quality embeddings for downstream tasks\n",
      "  - Fine-tuning capability for conditional protein binder generation (DSMppi)\n",
      "\n",
      "The architectural differences are significant: SaProt focuses on structure-aware tokenization while DSM uses diffusion-based generation. The documents supporting this are clearly identified in the metadata - SaProt document for its innovations, and DSM paper for its approach.\n",
      "\n",
      "I notice the context doesn't contain ProtElectra details at all, so I'll have to exclude that comparison strictly. The user seems knowledgeable about protein LLMs but wants precise, source-based comparisons - probably for research purposes. I'll make sure to cite the exact document names and metadata as requested.Based on the provided context, here's a comparison of SaProt with ProtT5, ESM-2, and DSM, including key architectural novelties:\n",
      "\n",
      "### **SaProt vs. ProtT5, ProtElectra, and ESM-2**\n",
      "1. **SaProt vs. ProtT5**  \n",
      "   - **Key Difference**: SaProt uses a \"structure-aware vocabulary\" combining residue tokens and structure-derived tokens (encoded via Foldseek), training on these unified sequences. ProtT5 performs *bilingual translation* between residue and structure tokens without combining them into a unified vocabulary.  \n",
      "   - **Performance**: SaProt consistently outperforms ProtT5 in protein function prediction tasks (e.g., ClinVar, DeepLoc) and zero-shot mutational effect prediction. ProtT5 has limitations as a general-purpose PLM ([SAPROT document](https://github.com/westlake-repl/SaProt)).  \n",
      "\n",
      "2. **SaProt vs. ProtElectra**  \n",
      "   - The context does not mention ProtElectra, so no comparison is possible.  \n",
      "\n",
      "3. **SaProt vs. ESM-2**  \n",
      "   - **Key Difference**: SaProt incorporates 3D structure information via structure-aware tokens, while ESM-2 relies solely on residue sequences.  \n",
      "   - **Performance**: SaProt outperforms ESM-2 in all protein-level tasks (e.g., Thermostability, HumanPPI) due to structure integration. It also surpasses structure-based models like MIF-ST, which use experimentally determined structures ([SAPROT document](https://github.com/westlake-repl/SaProt)).  \n",
      "\n",
      "---\n",
      "\n",
      "### **DSM vs. SaProt**\n",
      "1. **Core Architectural Innovation**  \n",
      "   DSM extends ESM-2 with a **masked diffusion process**, enabling:  \n",
      "   - *Generative capabilities*: High-quality sequence reconstruction even at 90% corruption (ASc ~0.27).  \n",
      "   - *Efficiency*: Realistic protein generation in a single forward pass, contrasting with iterative autoregressive or discrete diffusion methods.  \n",
      "   - *Embedding quality*: High-quality representations for downstream tasks, outperforming ESM-2 and DPLM of similar size ([Diffusion Sequence Models paper](arXiv:2506.08293v1)).  \n",
      "\n",
      "2. **Key Novelties of DSM**  \n",
      "   - **Masked Diffusion Head**: Replaces autoregressive modeling with a diffusion objective for sequence denoising.  \n",
      "   - **Conditional Generation**: Finetuning (DSMppi) enables template-guided protein binder design, surpassing known binders in affinity prediction.  \n",
      "   - **Unified Framework**: Combines representation learning and generative design, unlike SaProt (focused on structure-aware annotation) ([Diffusion Sequence Models paper](arXiv:2506.08293v1)).  \n",
      "\n",
      "---\n",
      "\n",
      "### **Source Documents Supporting the Answer**\n",
      "- **SaProt vs. ProtT5/ESM-2**:  \n",
      "  - *Document*: `SAPROT: PROTEIN LANGUAGE MODELING WITH STRUCTURE-AWARE VOCABULARY`  \n",
      "  - *Metadata*: Compares SaProt to ProtT5, ESM-2, and structure-based models (e.g., MIF-ST), highlighting SaProt’s superiority due to its unified vocabulary.  \n",
      "- **DSM vs. SaProt**:  \n",
      "  - *Document*: `Diffusion Sequence Models for Enhanced Protein Representation and Generation` (arXiv:2506.08293v1)  \n",
      "  - *Metadata*: Details DSM’s diffusion-based architecture, generative efficiency, and embedding quality against SaProt/ESM-2.  \n",
      "\n",
      "**Note**: ProtElectra is not discussed in the context, so no comparison is provided.\n",
      "\n",
      "=== SOURCES ===\n",
      "Content: percentage.\n",
      "Interestingly, the diffusion-based pLMs showcased a distinct sequence reconstruction scaling law that became clear when\n",
      "contrasting DSM and DPLM compared to ESM2 (Figure 4). Even at 90% ma...\n",
      "Metadata title: Diffusion Sequence Models for Enhanced Protein Representation and Generation\n",
      "\n",
      "Metadata page number: 6\n",
      "\n",
      "Content: percentage.\n",
      "Interestingly, the diffusion-based pLMs showcased a distinct sequence reconstruction scaling law that became clear when\n",
      "contrasting DSM and DPLM compared to ESM2 (Figure 4). Even at 90% ma...\n",
      "Metadata title: Diffusion Sequence Models for Enhanced Protein Representation and Generation\n",
      "\n",
      "Metadata page number: 6\n",
      "\n",
      "Content: percentage.\n",
      "Interestingly, the diffusion-based pLMs showcased a distinct sequence reconstruction scaling law that became clear when\n",
      "contrasting DSM and DPLM compared to ESM2 (Figure 4). Even at 90% ma...\n",
      "Metadata title: Diffusion Sequence Models for Enhanced Protein Representation and Generation\n",
      "\n",
      "Metadata page number: 6\n",
      "\n",
      "Content: embeddings and valuable downstream tasks (datasets described in Supplemental Figure S3). DSM650 produced the highest\n",
      "quality embeddings among similarly sized pLMs, generating consistently high F1 scor...\n",
      "Metadata title: Diffusion Sequence Models for Enhanced Protein Representation and Generation\n",
      "\n",
      "Metadata page number: 6\n",
      "\n",
      "Content: overtaken by the much larger ProtT5 on average (Figure 5). Importantly, we saw a boost in performance over its base weights of\n",
      "ESM2650, although all the model performances were much better than random...\n",
      "Metadata title: Diffusion Sequence Models for Enhanced Protein Representation and Generation\n",
      "\n",
      "Metadata page number: 6\n",
      "\n",
      "Content: Interestingly, the diffusion-based pLMs showcased a distinct sequence reconstruction scaling law that became clear when\n",
      "contrasting DSM and DPLM compared to ESM2 (Figure 4). Even at 90% masking, the d...\n",
      "Metadata title: Diffusion Sequence Models for Enhanced Protein Representation and Generation\n",
      "\n",
      "Metadata page number: 6\n",
      "\n",
      "Content: • SaProt outperforms ESM-2 in all protein-level tasks. Specifically, SaProt shows remark-\n",
      "able enhancements over ESM-2 in the Thermostability, HumanPPI, Metal Ion Binding, and\n",
      "DeepLoc tasks. This outc...\n",
      "Metadata title: SaProt: Protein Language Modeling with Structure-aware Vocabulary\n",
      "\n",
      "Metadata page number: 7\n",
      "\n",
      "Content: • SaProt outperforms ESM-2 in all protein-level tasks. Specifically, SaProt shows remark-\n",
      "able enhancements over ESM-2 in the Thermostability, HumanPPI, Metal Ion Binding, and\n",
      "DeepLoc tasks. This outc...\n",
      "Metadata title: SaProt: Protein Language Modeling with Structure-aware Vocabulary\n",
      "\n",
      "Metadata page number: 7\n",
      "\n",
      "Content: • SaProt outperforms ESM-2 in all protein-level tasks. Specifically, SaProt shows remark-\n",
      "able enhancements over ESM-2 in the Thermostability, HumanPPI, Metal Ion Binding, and\n",
      "DeepLoc tasks. This outc...\n",
      "Metadata title: SaProt: Protein Language Modeling with Structure-aware Vocabulary\n",
      "\n",
      "Metadata page number: 7\n",
      "\n",
      "Content: • SaProt outperforms ESM-2 in all protein-level tasks. Specifically, SaProt shows remark-\n",
      "able enhancements over ESM-2 in the Thermostability, HumanPPI, Metal Ion Binding, and\n",
      "DeepLoc tasks. This outc...\n",
      "Metadata title: SaProt: Protein Language Modeling with Structure-aware Vocabulary\n",
      "\n",
      "Metadata page number: 7\n",
      "\n",
      "Content: tokens. The sequential nature, rather than the graph 2 structure, of protein representation\n",
      "allows for seamless integration with advances in large-scale foundation AI models, such as\n",
      "BERT (Devlin et a...\n",
      "Metadata title: SaProt: Protein Language Modeling with Structure-aware Vocabulary\n",
      "\n",
      "Metadata page number: 1\n",
      "\n",
      "Content: allows for seamless integration with advances in large-scale foundation AI models, such as\n",
      "BERT (Devlin et al., 2018), BART (Lewis et al., 2019), GPT (Brown et al., 2020), etc.\n",
      "• By utilizing the SA-t...\n",
      "Metadata title: SaProt: Protein Language Modeling with Structure-aware Vocabulary\n",
      "\n",
      "Metadata page number: 1\n",
      "\n",
      "Content: allows for seamless integration with advances in large-scale foundation AI models, such as\n",
      "BERT (Devlin et al., 2018), BART (Lewis et al., 2019), GPT (Brown et al., 2020), etc.\n",
      "• By utilizing the SA-t...\n",
      "Metadata title: SaProt: Protein Language Modeling with Structure-aware Vocabulary\n",
      "\n",
      "Metadata page number: 1\n",
      "\n",
      "Content: allows for seamless integration with advances in large-scale foundation AI models, such as\n",
      "BERT (Devlin et al., 2018), BART (Lewis et al., 2019), GPT (Brown et al., 2020), etc.\n",
      "• By utilizing the SA-t...\n",
      "Metadata title: SaProt: Protein Language Modeling with Structure-aware Vocabulary\n",
      "\n",
      "Metadata page number: 1\n",
      "\n",
      "Content: allows for seamless integration with advances in large-scale foundation AI models, such as\n",
      "BERT (Devlin et al., 2018), BART (Lewis et al., 2019), GPT (Brown et al., 2020), etc.\n",
      "• By utilizing the SA-t...\n",
      "Metadata title: SaProt: Protein Language Modeling with Structure-aware Vocabulary\n",
      "\n",
      "Metadata page number: 1\n",
      "\n",
      "Content: • SaProt outperforms ESM-2 in all protein-level tasks. Specifically, SaProt shows remark-\n",
      "able enhancements over ESM-2 in the Thermostability, HumanPPI, Metal Ion Binding, and\n",
      "DeepLoc tasks. This outc...\n",
      "Metadata title: SaProt: Protein Language Modeling with Structure-aware Vocabulary\n",
      "\n",
      "Metadata page number: 7\n",
      "\n",
      "Content: arXiv:2506.08293v1  [q-bio.BM]  9 Jun 2025\n",
      "Diffusion Sequence Models for Enhanced Protein Representation and\n",
      "Generation\n",
      "Logan Hallee1, 2, Nikolaos Rafailidis1, David B. Bichara2, and Jason P. Gleghorn...\n",
      "Metadata title: Diffusion Sequence Models for Enhanced Protein Representation and Generation\n",
      "\n",
      "Metadata page number: 0\n",
      "\n",
      "Content: arXiv:2506.08293v1  [q-bio.BM]  9 Jun 2025\n",
      "Diffusion Sequence Models for Enhanced Protein Representation and\n",
      "Generation\n",
      "Logan Hallee1, 2, Nikolaos Rafailidis1, David B. Bichara2, and Jason P. Gleghorn...\n",
      "Metadata title: Diffusion Sequence Models for Enhanced Protein Representation and Generation\n",
      "\n",
      "Metadata page number: 0\n",
      "\n",
      "Content: arXiv:2506.08293v1  [q-bio.BM]  9 Jun 2025\n",
      "Diffusion Sequence Models for Enhanced Protein Representation and\n",
      "Generation\n",
      "Logan Hallee1, 2, Nikolaos Rafailidis1, David B. Bichara2, and Jason P. Gleghorn...\n",
      "Metadata title: Diffusion Sequence Models for Enhanced Protein Representation and Generation\n",
      "\n",
      "Metadata page number: 0\n",
      "\n",
      "Content: arXiv:2506.08293v1  [q-bio.BM]  9 Jun 2025\n",
      "Diffusion Sequence Models for Enhanced Protein Representation and\n",
      "Generation\n",
      "Logan Hallee1, 2, Nikolaos Rafailidis1, David B. Bichara2, and Jason P. Gleghorn...\n",
      "Metadata title: Diffusion Sequence Models for Enhanced Protein Representation and Generation\n",
      "\n",
      "Metadata page number: 0\n",
      "\n",
      "Content: tures. Through extensive evaluation, our SaProt model surpasses well-established\n",
      "and renowned baselines across 10 significant downstream tasks, demonstrating\n",
      "its exceptional capacity and broad applica...\n",
      "Metadata title: SaProt: Protein Language Modeling with Structure-aware Vocabulary\n",
      "\n",
      "Metadata page number: 0\n",
      "\n",
      "Content: sive dataset comprising approximately 40 million protein sequences and struc-\n",
      "tures. Through extensive evaluation, our SaProt model surpasses well-established\n",
      "and renowned baselines across 10 signific...\n",
      "Metadata title: SaProt: Protein Language Modeling with Structure-aware Vocabulary\n",
      "\n",
      "Metadata page number: 0\n",
      "\n",
      "Content: sive dataset comprising approximately 40 million protein sequences and struc-\n",
      "tures. Through extensive evaluation, our SaProt model surpasses well-established\n",
      "and renowned baselines across 10 signific...\n",
      "Metadata title: SaProt: Protein Language Modeling with Structure-aware Vocabulary\n",
      "\n",
      "Metadata page number: 0\n",
      "\n",
      "Content: sive dataset comprising approximately 40 million protein sequences and struc-\n",
      "tures. Through extensive evaluation, our SaProt model surpasses well-established\n",
      "and renowned baselines across 10 signific...\n",
      "Metadata title: SaProt: Protein Language Modeling with Structure-aware Vocabulary\n",
      "\n",
      "Metadata page number: 0\n",
      "\n",
      "Content: prediction tasks (Meier et al., 2021). For dataset details, we refer readers to Appendix D.2.1.\n",
      "4.1.2 B ASELINES & EVALUATION\n",
      "We compare SaProt with two types of baselines: sequence-based models and s...\n",
      "Metadata title: SaProt: Protein Language Modeling with Structure-aware Vocabulary\n",
      "\n",
      "Metadata page number: 5\n",
      "\n",
      "Content: prediction tasks (Meier et al., 2021). For dataset details, we refer readers to Appendix D.2.1.\n",
      "4.1.2 B ASELINES & EVALUATION\n",
      "We compare SaProt with two types of baselines: sequence-based models and s...\n",
      "Metadata title: SaProt: Protein Language Modeling with Structure-aware Vocabulary\n",
      "\n",
      "Metadata page number: 5\n",
      "\n",
      "Content: prediction tasks (Meier et al., 2021). For dataset details, we refer readers to Appendix D.2.1.\n",
      "4.1.2 B ASELINES & EVALUATION\n",
      "We compare SaProt with two types of baselines: sequence-based models and s...\n",
      "Metadata title: SaProt: Protein Language Modeling with Structure-aware Vocabulary\n",
      "\n",
      "Metadata page number: 5\n",
      "\n",
      "Content: 4.1.3 R ESULTS\n",
      "Table 1 showcases the zero-shot results on the ProteinGym and ClinVar datasets. Based on the\n",
      "results, we draw several noteworthy conclusions:\n",
      "• SaProt outperforms all residue sequence-b...\n",
      "Metadata title: SaProt: Protein Language Modeling with Structure-aware Vocabulary\n",
      "\n",
      "Metadata page number: 6\n",
      "\n",
      "Content: 4.1.3 R ESULTS\n",
      "Table 1 showcases the zero-shot results on the ProteinGym and ClinVar datasets. Based on the\n",
      "results, we draw several noteworthy conclusions:\n",
      "• SaProt outperforms all residue sequence-b...\n",
      "Metadata title: SaProt: Protein Language Modeling with Structure-aware Vocabulary\n",
      "\n",
      "Metadata page number: 6\n",
      "\n",
      "Content: 4.1.3 R ESULTS\n",
      "Table 1 showcases the zero-shot results on the ProteinGym and ClinVar datasets. Based on the\n",
      "results, we draw several noteworthy conclusions:\n",
      "• SaProt outperforms all residue sequence-b...\n",
      "Metadata title: SaProt: Protein Language Modeling with Structure-aware Vocabulary\n",
      "\n",
      "Metadata page number: 6\n",
      "\n",
      "Content: 4.1.3 R ESULTS\n",
      "Table 1 showcases the zero-shot results on the ProteinGym and ClinVar datasets. Based on the\n",
      "results, we draw several noteworthy conclusions:\n",
      "• SaProt outperforms all residue sequence-b...\n",
      "Metadata title: SaProt: Protein Language Modeling with Structure-aware Vocabulary\n",
      "\n",
      "Metadata page number: 6\n",
      "\n",
      "Content: process. DSM extends ESM2 ( 33) with a novel language modeling head and training objective, enabling robust denoising\n",
      "across high corruption rates and sequence generation with global context. After tr...\n",
      "Metadata title: Diffusion Sequence Models for Enhanced Protein Representation and Generation\n",
      "\n",
      "Metadata page number: 1\n",
      "\n",
      "Content: process. DSM extends ESM2 ( 33) with a novel language modeling head and training objective, enabling robust denoising\n",
      "across high corruption rates and sequence generation with global context. After tr...\n",
      "Metadata title: Diffusion Sequence Models for Enhanced Protein Representation and Generation\n",
      "\n",
      "Metadata page number: 1\n",
      "\n",
      "Content: process. DSM extends ESM2 ( 33) with a novel language modeling head and training objective, enabling robust denoising\n",
      "across high corruption rates and sequence generation with global context. After tr...\n",
      "Metadata title: Diffusion Sequence Models for Enhanced Protein Representation and Generation\n",
      "\n",
      "Metadata page number: 1\n",
      "\n",
      "Content: process. DSM extends ESM2 ( 33) with a novel language modeling head and training objective, enabling robust denoising\n",
      "across high corruption rates and sequence generation with global context. After tr...\n",
      "Metadata title: Diffusion Sequence Models for Enhanced Protein Representation and Generation\n",
      "\n",
      "Metadata page number: 1\n",
      "\n",
      "Content: process. DSM extends ESM2 ( 33) with a novel language modeling head and training objective, enabling robust denoising\n",
      "across high corruption rates and sequence generation with global context. After tr...\n",
      "Metadata title: Diffusion Sequence Models for Enhanced Protein Representation and Generation\n",
      "\n",
      "Metadata page number: 1\n",
      "\n",
      "Content: enabling the transformation of the residue sequence into an SA-token sequence. SaProt is trained\n",
      "on these new token sequences, effectively incorporating structure information. One acknowledged\n",
      "drawbac...\n",
      "Metadata title: SaProt: Protein Language Modeling with Structure-aware Vocabulary\n",
      "\n",
      "Metadata page number: 14\n",
      "\n",
      "Content: 5 Conclusion\n",
      "We introduced Diffusion Sequence Modeling (DSM), a simple yet powerful way to retrofit any MLM-based pLM with a\n",
      "diffusion objective. With only minor modifications to the masking scheme, l...\n",
      "Metadata title: Diffusion Sequence Models for Enhanced Protein Representation and Generation\n",
      "\n",
      "Metadata page number: 7\n",
      "\n",
      "Content: 5 Conclusion\n",
      "We introduced Diffusion Sequence Modeling (DSM), a simple yet powerful way to retrofit any MLM-based pLM with a\n",
      "diffusion objective. With only minor modifications to the masking scheme, l...\n",
      "Metadata title: Diffusion Sequence Models for Enhanced Protein Representation and Generation\n",
      "\n",
      "Metadata page number: 7\n",
      "\n",
      "Content: 5 Conclusion\n",
      "We introduced Diffusion Sequence Modeling (DSM), a simple yet powerful way to retrofit any MLM-based pLM with a\n",
      "diffusion objective. With only minor modifications to the masking scheme, l...\n",
      "Metadata title: Diffusion Sequence Models for Enhanced Protein Representation and Generation\n",
      "\n",
      "Metadata page number: 7\n",
      "\n",
      "Content: the LLaDA framework. After training, DSM is capable of generating diverse, biomimetic sequences that align with expected amino\n",
      "acid compositions, secondary structures, and predicted functions, even wi...\n",
      "Metadata title: Diffusion Sequence Models for Enhanced Protein Representation and Generation\n",
      "\n",
      "Metadata page number: 0\n",
      "\n",
      "Content: We compare SaProt with two types of baselines: sequence-based models and structure-based mod-\n",
      "els. For sequence-based models, we include ESM-1b (Rives et al., 2019), ESM-1v (Meier et al.,\n",
      "2021) (the r...\n",
      "Metadata title: SaProt: Protein Language Modeling with Structure-aware Vocabulary\n",
      "\n",
      "Metadata page number: 5\n",
      "\n",
      "Content: SAPROT: P ROTEIN LANGUAGE MODELING WITH\n",
      "STRUCTURE -AWARE VOCABULARY\n",
      "Jin Su, Chenchen Han, Yuyang Zhou, Junjie Shan, Xibin Zhou, Fajie Yuan∗\n",
      "Westlake University\n",
      "{sujin, hanchenchen, zhouyuyang, shanjun...\n",
      "Metadata title: SaProt: Protein Language Modeling with Structure-aware Vocabulary\n",
      "\n",
      "Metadata page number: 0\n",
      "\n",
      "Content: SAPROT: P ROTEIN LANGUAGE MODELING WITH\n",
      "STRUCTURE -AWARE VOCABULARY\n",
      "Jin Su, Chenchen Han, Yuyang Zhou, Junjie Shan, Xibin Zhou, Fajie Yuan∗\n",
      "Westlake University\n",
      "{sujin, hanchenchen, zhouyuyang, shanjun...\n",
      "Metadata title: SaProt: Protein Language Modeling with Structure-aware Vocabulary\n",
      "\n",
      "Metadata page number: 0\n",
      "\n",
      "Content: SAPROT: P ROTEIN LANGUAGE MODELING WITH\n",
      "STRUCTURE -AWARE VOCABULARY\n",
      "Jin Su, Chenchen Han, Yuyang Zhou, Junjie Shan, Xibin Zhou, Fajie Yuan∗\n",
      "Westlake University\n",
      "{sujin, hanchenchen, zhouyuyang, shanjun...\n",
      "Metadata title: SaProt: Protein Language Modeling with Structure-aware Vocabulary\n",
      "\n",
      "Metadata page number: 0\n",
      "\n",
      "Content: SAPROT: P ROTEIN LANGUAGE MODELING WITH\n",
      "STRUCTURE -AWARE VOCABULARY\n",
      "Jin Su, Chenchen Han, Yuyang Zhou, Junjie Shan, Xibin Zhou, Fajie Yuan∗\n",
      "Westlake University\n",
      "{sujin, hanchenchen, zhouyuyang, shanjun...\n",
      "Metadata title: SaProt: Protein Language Modeling with Structure-aware Vocabulary\n",
      "\n",
      "Metadata page number: 0\n",
      "\n",
      "Content: A C OMPARISON WITH PROST T5\n",
      "A very recent preprint in Heinzinger et al. (2023) proposed ProstT5 which pre-trains the protein\n",
      "language model (PLM) on a mixture of data of Foldseek token sequences and r...\n",
      "Metadata title: SaProt: Protein Language Modeling with Structure-aware Vocabulary\n",
      "\n",
      "Metadata page number: 14\n",
      "\n",
      "Content: A C OMPARISON WITH PROST T5\n",
      "A very recent preprint in Heinzinger et al. (2023) proposed ProstT5 which pre-trains the protein\n",
      "language model (PLM) on a mixture of data of Foldseek token sequences and r...\n",
      "Metadata title: SaProt: Protein Language Modeling with Structure-aware Vocabulary\n",
      "\n",
      "Metadata page number: 14\n",
      "\n",
      "Content: A C OMPARISON WITH PROST T5\n",
      "A very recent preprint in Heinzinger et al. (2023) proposed ProstT5 which pre-trains the protein\n",
      "language model (PLM) on a mixture of data of Foldseek token sequences and r...\n",
      "Metadata title: SaProt: Protein Language Modeling with Structure-aware Vocabulary\n",
      "\n",
      "Metadata page number: 14\n",
      "\n",
      "Content: A C OMPARISON WITH PROST T5\n",
      "A very recent preprint in Heinzinger et al. (2023) proposed ProstT5 which pre-trains the protein\n",
      "language model (PLM) on a mixture of data of Foldseek token sequences and r...\n",
      "Metadata title: SaProt: Protein Language Modeling with Structure-aware Vocabulary\n",
      "\n",
      "Metadata page number: 14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== ANSWER ===\")\n",
    "print(result['result'])   \n",
    "print(\"\\n=== SOURCES ===\")\n",
    "for doc in result['source_documents']:\n",
    "    print(f\"Content: {doc.page_content[:200]}...\")\n",
    "    print(f\"Metadata title: {doc.metadata['title']}\\n\")\n",
    "    print(f\"Metadata page number: {doc.metadata['page']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecae96d",
   "metadata": {},
   "source": [
    "__Check Retrieved Documents, before the LLM stage, confirm retrieval works:__\n",
    "* This is purely a vector-based search using embeddings. Your query is converted into a numerical vector using an embedding model\n",
    "* The vector database (Chroma, FAISS, etc.) compares your query's embedding against all stored document embeddings using cosine similarity.\n",
    "* The system returns the k documents with the highest similarity scores (\"semantic search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc34138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Retrieved Documents\n",
    "\n",
    "retrived_docs = my_retriever.invoke(\"SaProt LLM model PLM SA-token structure-aware vocabulary\")\n",
    "print(f\"Retrieved {len(retrived_docs)} docs:\")\n",
    "for doc in retrived_docs:\n",
    "    print(f'\\n====== Metedata of the chunk======:\\n{doc.metadata['title']}\\n')\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "99aa7dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 40 docs:\n",
      "\n",
      "====== Metedata of the chunk======:\n",
      "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning\n",
      "\n",
      "to thank Nicolas Castet and Bryant Nelson for their help to\n",
      "ﬁx issues and enhance the performance of IBM PowerAI.\n",
      "From Google, the authors would like to thank Jamie Kinney,\n",
      "Alex Schroeder, Nicole DeSantis, Andrew Stein, Vishal Mis-\n",
      "hra, Eleazar Ortiz, Nora Limbourg, Cristian Mezzanotte,\n",
      "and all TFRC Team for helping to setup a project on Google\n",
      "Cloud and solving Google cloud issues. No ProtTrans\n",
      "model was easily publicly available without support from\n",
      "the Hugging Face team, including Patrick von Platen, Julien\n",
      "Chaumond, and Clement Delangue. The authors would\n",
      "also like to thank Konstantin Weißenow for helping with\n",
      "grant writing and providing early results for the structure\n",
      "prediction task. The authors would also like to thank both\n",
      "Adam Roberts and Colin Raffel for help with the T5 model,\n",
      "and the editor and the anonymous reviewers for essential\n",
      "criticism, especially, for suggesting to compare t-SNEs to\n",
      "randomly initialized models. The authors would also like to\n",
      "thank Leibniz Rechenzentrum (LRZ) for providing access\n",
      "7124 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 44, NO. 10, OCTOBER 2022\n",
      "\n",
      "====== Metedata of the chunk======:\n",
      "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning\n",
      "\n",
      "layers which reduced the global batch size to 1024. Due to\n",
      "the relatively small batch-size, we used the original opti-\n",
      "mizer: Adam with a learning rate of 0.00001. The model\n",
      "was trained through more steps, i.e., 20k warm-up and\n",
      "847k steps to compensate for the smaller batch-size of this\n",
      "model.\n",
      "ProtElectra. Electra5 consists of two models, a generator\n",
      "and discriminator (same number of layers, generator 25\n",
      "percent of the discriminator’s hidden layer size, hidden\n",
      "layer intermediate size, and number of heads). We copied\n",
      "Electra’s NLP conﬁguration with two changes: increasing\n",
      "the number of layers to 30 and using Lamb optimizer.\n",
      "Again, we split the training into two phases: the ﬁrst for\n",
      "proteins /C20512 residues (400k steps at 9k global batch size),\n",
      "\n",
      "====== Metedata of the chunk======:\n",
      "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning\n",
      "\n",
      "5. https://github.com/google-research/electra\n",
      "6. https://github.com/google-research/text-to-text-transfer-\n",
      "transformer\n",
      "7116 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 44, NO. 10, OCTOBER 2022\n",
      "\n",
      "====== Metedata of the chunk======:\n",
      "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning\n",
      "\n",
      "[15] using evolutionary information (Fig. 4 and SOM Tables\n",
      "7, 6, available online). However, ProtT5-XL-U50 reached\n",
      "nearly identical performance without ever using multiple\n",
      "sequence alignments (MSA). Analyzing the average Q3 per\n",
      "protein of both models for set NEW364 in more detail (SOM\n",
      "Fig. 12, available online), revealed that 57 percent of the\n",
      "ELNAGGAR ET AL.: PROTTRANS: TOWARD UNDERSTANDING THE LANGUAGE OF LIFE THROUGH SELF-SUPERVISED LEARNING 7119\n",
      "\n",
      "====== Metedata of the chunk======:\n",
      "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning\n",
      "\n",
      "and SOM Table 6, available online). Both ProtTXL versions\n",
      "fell short compared to an existing ELMo/LSTM-based solu-\n",
      "tion (DeepSeqVec [56]) while all other Transformer-models\n",
      "outperformed DeepSeqVec. Embeddings extracted from\n",
      "another large Transformer (ESMB-1b [67]), improved over\n",
      "all our non-ProtT5 models (Fig. 4 and SOM Table 6, available\n",
      "online). Most solutions using only embeddings as input\n",
      "were outperformed by the top SOA method NetSurfP-2.0\n",
      "[15] using evolutionary information (Fig. 4 and SOM Tables\n",
      "7, 6, available online). However, ProtT5-XL-U50 reached\n",
      "nearly identical performance without ever using multiple\n",
      "sequence alignments (MSA). Analyzing the average Q3 per\n",
      "protein of both models for set NEW364 in more detail (SOM\n",
      "Fig. 12, available online), revealed that 57 percent of the\n",
      "\n",
      "====== Metedata of the chunk======:\n",
      "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning\n",
      "\n",
      "tokens, potentially creating plausible alternatives, and the\n",
      "discriminator (Electra) detects which tokens were masked.\n",
      "This enriches the training signal as the loss can be computed\n",
      "over all tokens instead of the subset of corrupted tokens\n",
      "(usually only 15 percent).T5 uses the original transformer\n",
      "architecture proposed for sequence translation, which con-\n",
      "sists of an encoder that projects a source language to an\n",
      "embedding space and a decoder that generates a translation\n",
      "to a target language based on the encoder’s embedding.\n",
      "Only later, models used either the encoder (BERT, Albert,\n",
      "Electra) or the decoder ( TransformerXL, XLNet), but T5\n",
      "showed that this simpliﬁcation might come at a certain price\n",
      "as it reaches state-of-the-art results in multiple NLP bench-\n",
      "\n",
      "====== Metedata of the chunk======:\n",
      "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning\n",
      "\n",
      "percent of the discriminator’s hidden layer size, hidden\n",
      "layer intermediate size, and number of heads). We copied\n",
      "Electra’s NLP conﬁguration with two changes: increasing\n",
      "the number of layers to 30 and using Lamb optimizer.\n",
      "Again, we split the training into two phases: the ﬁrst for\n",
      "proteins /C20512 residues (400k steps at 9k global batch size),\n",
      "the second for proteins /C201024 (400k steps at 3.5k global\n",
      "batch size). While ProtTXL, ProtBert, ProtAlbert and\n",
      "ProtXLNet relied on pre-computed tensorﬂow records as\n",
      "input, Electra allowed to mask sequences on the ﬂy, allow-\n",
      "ing the model to see different masking patterns during\n",
      "each epoch.\n",
      "ProtT5. Unlike the previous LMs, T56 uses an encoder\n",
      "and decoder [10]. We trained two model sizes, one with 3B\n",
      "(T5-XL) and one with 11B parameters (T5-XXL). T5-XL was\n",
      "trained using 8-way model parallelism, while T5-XXL was\n",
      "trained using 32-way model parallelism. First, T5-XL and\n",
      "T5-XXL were trained on BFD for 1.2M and 920k steps\n",
      "respectively (ProtT5-XL-BFD, ProtT5-XXL-BFD). In a second\n",
      "step, ProtT5-XL-BFD and ProtT5-XXL-BFD were ﬁne-tuned\n",
      "on UniRef50 for 991k and 343k steps respectively (ProtT5-\n",
      "XL-U50, ProtT5-XXL-U50). Contrary to the original T5\n",
      "\n",
      "====== Metedata of the chunk======:\n",
      "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning\n",
      "\n",
      "Again, we split the training into two phases: the ﬁrst for\n",
      "proteins /C20512 residues (400k steps at 9k global batch size),\n",
      "the second for proteins /C201024 (400k steps at 3.5k global\n",
      "batch size). While ProtTXL, ProtBert, ProtAlbert and\n",
      "ProtXLNet relied on pre-computed tensorﬂow records as\n",
      "input, Electra allowed to mask sequences on the ﬂy, allow-\n",
      "ing the model to see different masking patterns during\n",
      "each epoch.\n",
      "ProtT5. Unlike the previous LMs, T56 uses an encoder\n",
      "and decoder [10]. We trained two model sizes, one with 3B\n",
      "(T5-XL) and one with 11B parameters (T5-XXL). T5-XL was\n",
      "trained using 8-way model parallelism, while T5-XXL was\n",
      "trained using 32-way model parallelism. First, T5-XL and\n",
      "T5-XXL were trained on BFD for 1.2M and 920k steps\n",
      "\n",
      "====== Metedata of the chunk======:\n",
      "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning\n",
      "\n",
      "4. https://github.com/zihangdai/xlnet\n",
      "5. https://github.com/google-research/electra\n",
      "6. https://github.com/google-research/text-to-text-transfer-\n",
      "transformer\n",
      "7116 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 44, NO. 10, OCTOBER 2022\n",
      "\n",
      "====== Metedata of the chunk======:\n",
      "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning\n",
      "\n",
      "publication, we achieved increasing the global batch size\n",
      "from 4096 to 10752 on the same hardware. The reason for\n",
      "this counter-intuitive effect is the reduced vocabulary size\n",
      "in proteins: the entire diversity of the protein universe is\n",
      "realized by 20 different amino acids, compared to tens of\n",
      "thousands of different words. Similar to ProtBert, ProtAl-\n",
      "bert was ﬁrst trained for 150k steps on sequences with a\n",
      "maximum length of 512 and then for another 150k steps on\n",
      "sequences with a maximum length of 2k.\n",
      "ProtXLNet. XLNet4 was trained on UniRef100 (ProtXL-\n",
      "Net) using the original NLP conﬁguration [11] (Table 2)\n",
      "except for the number of layers that was increased to 30\n",
      "layers which reduced the global batch size to 1024. Due to\n",
      "the relatively small batch-size, we used the original opti-\n",
      "mizer: Adam with a learning rate of 0.00001. The model\n",
      "was trained through more steps, i.e., 20k warm-up and\n",
      "847k steps to compensate for the smaller batch-size of this\n",
      "model.\n",
      "ProtElectra. Electra5 consists of two models, a generator\n",
      "and discriminator (same number of layers, generator 25\n",
      "percent of the discriminator’s hidden layer size, hidden\n",
      "layer intermediate size, and number of heads). We copied\n",
      "\n",
      "====== Metedata of the chunk======:\n",
      "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning\n",
      "\n",
      "publication, we achieved increasing the global batch size\n",
      "from 4096 to 10752 on the same hardware. The reason for\n",
      "this counter-intuitive effect is the reduced vocabulary size\n",
      "in proteins: the entire diversity of the protein universe is\n",
      "realized by 20 different amino acids, compared to tens of\n",
      "thousands of different words. Similar to ProtBert, ProtAl-\n",
      "bert was ﬁrst trained for 150k steps on sequences with a\n",
      "maximum length of 512 and then for another 150k steps on\n",
      "sequences with a maximum length of 2k.\n",
      "ProtXLNet. XLNet4 was trained on UniRef100 (ProtXL-\n",
      "Net) using the original NLP conﬁguration [11] (Table 2)\n",
      "except for the number of layers that was increased to 30\n",
      "layers which reduced the global batch size to 1024. Due to\n",
      "the relatively small batch-size, we used the original opti-\n",
      "mizer: Adam with a learning rate of 0.00001. The model\n",
      "was trained through more steps, i.e., 20k warm-up and\n",
      "847k steps to compensate for the smaller batch-size of this\n",
      "model.\n",
      "ProtElectra. Electra5 consists of two models, a generator\n",
      "and discriminator (same number of layers, generator 25\n",
      "percent of the discriminator’s hidden layer size, hidden\n",
      "layer intermediate size, and number of heads). We copied\n",
      "\n",
      "====== Metedata of the chunk======:\n",
      "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning\n",
      "\n",
      "publication, we achieved increasing the global batch size\n",
      "from 4096 to 10752 on the same hardware. The reason for\n",
      "this counter-intuitive effect is the reduced vocabulary size\n",
      "in proteins: the entire diversity of the protein universe is\n",
      "realized by 20 different amino acids, compared to tens of\n",
      "thousands of different words. Similar to ProtBert, ProtAl-\n",
      "bert was ﬁrst trained for 150k steps on sequences with a\n",
      "maximum length of 512 and then for another 150k steps on\n",
      "sequences with a maximum length of 2k.\n",
      "ProtXLNet. XLNet4 was trained on UniRef100 (ProtXL-\n",
      "Net) using the original NLP conﬁguration [11] (Table 2)\n",
      "except for the number of layers that was increased to 30\n",
      "layers which reduced the global batch size to 1024. Due to\n",
      "the relatively small batch-size, we used the original opti-\n",
      "mizer: Adam with a learning rate of 0.00001. The model\n",
      "was trained through more steps, i.e., 20k warm-up and\n",
      "847k steps to compensate for the smaller batch-size of this\n",
      "model.\n",
      "ProtElectra. Electra5 consists of two models, a generator\n",
      "and discriminator (same number of layers, generator 25\n",
      "percent of the discriminator’s hidden layer size, hidden\n",
      "layer intermediate size, and number of heads). We copied\n",
      "\n",
      "====== Metedata of the chunk======:\n",
      "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning\n",
      "\n",
      "publication, we achieved increasing the global batch size\n",
      "from 4096 to 10752 on the same hardware. The reason for\n",
      "this counter-intuitive effect is the reduced vocabulary size\n",
      "in proteins: the entire diversity of the protein universe is\n",
      "realized by 20 different amino acids, compared to tens of\n",
      "thousands of different words. Similar to ProtBert, ProtAl-\n",
      "bert was ﬁrst trained for 150k steps on sequences with a\n",
      "maximum length of 512 and then for another 150k steps on\n",
      "sequences with a maximum length of 2k.\n",
      "ProtXLNet. XLNet4 was trained on UniRef100 (ProtXL-\n",
      "Net) using the original NLP conﬁguration [11] (Table 2)\n",
      "except for the number of layers that was increased to 30\n",
      "layers which reduced the global batch size to 1024. Due to\n",
      "the relatively small batch-size, we used the original opti-\n",
      "mizer: Adam with a learning rate of 0.00001. The model\n",
      "was trained through more steps, i.e., 20k warm-up and\n",
      "847k steps to compensate for the smaller batch-size of this\n",
      "model.\n",
      "ProtElectra. Electra5 consists of two models, a generator\n",
      "and discriminator (same number of layers, generator 25\n",
      "percent of the discriminator’s hidden layer size, hidden\n",
      "layer intermediate size, and number of heads). We copied\n",
      "\n",
      "====== Metedata of the chunk======:\n",
      "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning\n",
      "\n",
      "eric Pariente, Jonathan Lefman, and Thomas Bradley, and\n",
      "many at ORNL without whom no aspect of this work could\n",
      "have been realized, particular thanks to John Gounley,\n",
      "Hong-Jun Yoon, Georgia Tourassi, Bill, Brian, Junqi, Gra-\n",
      "ham, and Ver/C19onica (ORNL Summit). The authors would\n",
      "also like to thank Jack Wells (ORNL) for opening the door\n",
      "to kicking off this project. From IBM, the authors would like\n",
      "to thank Nicolas Castet and Bryant Nelson for their help to\n",
      "ﬁx issues and enhance the performance of IBM PowerAI.\n",
      "From Google, the authors would like to thank Jamie Kinney,\n",
      "Alex Schroeder, Nicole DeSantis, Andrew Stein, Vishal Mis-\n",
      "hra, Eleazar Ortiz, Nora Limbourg, Cristian Mezzanotte,\n",
      "and all TFRC Team for helping to setup a project on Google\n",
      "Cloud and solving Google cloud issues. No ProtTrans\n",
      "model was easily publicly available without support from\n",
      "the Hugging Face team, including Patrick von Platen, Julien\n",
      "Chaumond, and Clement Delangue. The authors would\n",
      "also like to thank Konstantin Weißenow for helping with\n",
      "grant writing and providing early results for the structure\n",
      "prediction task. The authors would also like to thank both\n",
      "Adam Roberts and Colin Raffel for help with the T5 model,\n",
      "\n",
      "====== Metedata of the chunk======:\n",
      "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning\n",
      "\n",
      "eric Pariente, Jonathan Lefman, and Thomas Bradley, and\n",
      "many at ORNL without whom no aspect of this work could\n",
      "have been realized, particular thanks to John Gounley,\n",
      "Hong-Jun Yoon, Georgia Tourassi, Bill, Brian, Junqi, Gra-\n",
      "ham, and Ver/C19onica (ORNL Summit). The authors would\n",
      "also like to thank Jack Wells (ORNL) for opening the door\n",
      "to kicking off this project. From IBM, the authors would like\n",
      "to thank Nicolas Castet and Bryant Nelson for their help to\n",
      "ﬁx issues and enhance the performance of IBM PowerAI.\n",
      "From Google, the authors would like to thank Jamie Kinney,\n",
      "Alex Schroeder, Nicole DeSantis, Andrew Stein, Vishal Mis-\n",
      "hra, Eleazar Ortiz, Nora Limbourg, Cristian Mezzanotte,\n",
      "and all TFRC Team for helping to setup a project on Google\n",
      "Cloud and solving Google cloud issues. No ProtTrans\n",
      "model was easily publicly available without support from\n",
      "the Hugging Face team, including Patrick von Platen, Julien\n",
      "Chaumond, and Clement Delangue. The authors would\n",
      "also like to thank Konstantin Weißenow for helping with\n",
      "grant writing and providing early results for the structure\n",
      "prediction task. The authors would also like to thank both\n",
      "Adam Roberts and Colin Raffel for help with the T5 model,\n",
      "\n",
      "====== Metedata of the chunk======:\n",
      "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning\n",
      "\n",
      "eric Pariente, Jonathan Lefman, and Thomas Bradley, and\n",
      "many at ORNL without whom no aspect of this work could\n",
      "have been realized, particular thanks to John Gounley,\n",
      "Hong-Jun Yoon, Georgia Tourassi, Bill, Brian, Junqi, Gra-\n",
      "ham, and Ver/C19onica (ORNL Summit). The authors would\n",
      "also like to thank Jack Wells (ORNL) for opening the door\n",
      "to kicking off this project. From IBM, the authors would like\n",
      "to thank Nicolas Castet and Bryant Nelson for their help to\n",
      "ﬁx issues and enhance the performance of IBM PowerAI.\n",
      "From Google, the authors would like to thank Jamie Kinney,\n",
      "Alex Schroeder, Nicole DeSantis, Andrew Stein, Vishal Mis-\n",
      "hra, Eleazar Ortiz, Nora Limbourg, Cristian Mezzanotte,\n",
      "and all TFRC Team for helping to setup a project on Google\n",
      "Cloud and solving Google cloud issues. No ProtTrans\n",
      "model was easily publicly available without support from\n",
      "the Hugging Face team, including Patrick von Platen, Julien\n",
      "Chaumond, and Clement Delangue. The authors would\n",
      "also like to thank Konstantin Weißenow for helping with\n",
      "grant writing and providing early results for the structure\n",
      "prediction task. The authors would also like to thank both\n",
      "Adam Roberts and Colin Raffel for help with the T5 model,\n",
      "\n",
      "====== Metedata of the chunk======:\n",
      "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning\n",
      "\n",
      "fell short compared to an existing ELMo/LSTM-based solu-\n",
      "tion (DeepSeqVec [56]) while all other Transformer-models\n",
      "outperformed DeepSeqVec. Embeddings extracted from\n",
      "another large Transformer (ESMB-1b [67]), improved over\n",
      "all our non-ProtT5 models (Fig. 4 and SOM Table 6, available\n",
      "online). Most solutions using only embeddings as input\n",
      "were outperformed by the top SOA method NetSurfP-2.0\n",
      "[15] using evolutionary information (Fig. 4 and SOM Tables\n",
      "7, 6, available online). However, ProtT5-XL-U50 reached\n",
      "nearly identical performance without ever using multiple\n",
      "sequence alignments (MSA). Analyzing the average Q3 per\n",
      "protein of both models for set NEW364 in more detail (SOM\n",
      "Fig. 12, available online), revealed that 57 percent of the\n",
      "ELNAGGAR ET AL.: PROTTRANS: TOWARD UNDERSTANDING THE LANGUAGE OF LIFE THROUGH SELF-SUPERVISED LEARNING 7119\n",
      "\n",
      "====== Metedata of the chunk======:\n",
      "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning\n",
      "\n",
      "fell short compared to an existing ELMo/LSTM-based solu-\n",
      "tion (DeepSeqVec [56]) while all other Transformer-models\n",
      "outperformed DeepSeqVec. Embeddings extracted from\n",
      "another large Transformer (ESMB-1b [67]), improved over\n",
      "all our non-ProtT5 models (Fig. 4 and SOM Table 6, available\n",
      "online). Most solutions using only embeddings as input\n",
      "were outperformed by the top SOA method NetSurfP-2.0\n",
      "[15] using evolutionary information (Fig. 4 and SOM Tables\n",
      "7, 6, available online). However, ProtT5-XL-U50 reached\n",
      "nearly identical performance without ever using multiple\n",
      "sequence alignments (MSA). Analyzing the average Q3 per\n",
      "protein of both models for set NEW364 in more detail (SOM\n",
      "Fig. 12, available online), revealed that 57 percent of the\n",
      "ELNAGGAR ET AL.: PROTTRANS: TOWARD UNDERSTANDING THE LANGUAGE OF LIFE THROUGH SELF-SUPERVISED LEARNING 7119\n",
      "\n",
      "====== Metedata of the chunk======:\n",
      "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning\n",
      "\n",
      "fell short compared to an existing ELMo/LSTM-based solu-\n",
      "tion (DeepSeqVec [56]) while all other Transformer-models\n",
      "outperformed DeepSeqVec. Embeddings extracted from\n",
      "another large Transformer (ESMB-1b [67]), improved over\n",
      "all our non-ProtT5 models (Fig. 4 and SOM Table 6, available\n",
      "online). Most solutions using only embeddings as input\n",
      "were outperformed by the top SOA method NetSurfP-2.0\n",
      "[15] using evolutionary information (Fig. 4 and SOM Tables\n",
      "7, 6, available online). However, ProtT5-XL-U50 reached\n",
      "nearly identical performance without ever using multiple\n",
      "sequence alignments (MSA). Analyzing the average Q3 per\n",
      "protein of both models for set NEW364 in more detail (SOM\n",
      "Fig. 12, available online), revealed that 57 percent of the\n",
      "ELNAGGAR ET AL.: PROTTRANS: TOWARD UNDERSTANDING THE LANGUAGE OF LIFE THROUGH SELF-SUPERVISED LEARNING 7119\n",
      "\n",
      "====== Metedata of the chunk======:\n",
      "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning\n",
      "\n",
      "and discriminator (same number of layers, generator 25\n",
      "percent of the discriminator’s hidden layer size, hidden\n",
      "layer intermediate size, and number of heads). We copied\n",
      "Electra’s NLP conﬁguration with two changes: increasing\n",
      "the number of layers to 30 and using Lamb optimizer.\n",
      "Again, we split the training into two phases: the ﬁrst for\n",
      "proteins /C20512 residues (400k steps at 9k global batch size),\n",
      "the second for proteins /C201024 (400k steps at 3.5k global\n",
      "batch size). While ProtTXL, ProtBert, ProtAlbert and\n",
      "ProtXLNet relied on pre-computed tensorﬂow records as\n",
      "input, Electra allowed to mask sequences on the ﬂy, allow-\n",
      "ing the model to see different masking patterns during\n",
      "each epoch.\n",
      "ProtT5. Unlike the previous LMs, T56 uses an encoder\n",
      "and decoder [10]. We trained two model sizes, one with 3B\n",
      "(T5-XL) and one with 11B parameters (T5-XXL). T5-XL was\n",
      "trained using 8-way model parallelism, while T5-XXL was\n",
      "trained using 32-way model parallelism. First, T5-XL and\n",
      "T5-XXL were trained on BFD for 1.2M and 920k steps\n",
      "respectively (ProtT5-XL-BFD, ProtT5-XXL-BFD). In a second\n",
      "step, ProtT5-XL-BFD and ProtT5-XXL-BFD were ﬁne-tuned\n",
      "on UniRef50 for 991k and 343k steps respectively (ProtT5-\n",
      "\n",
      "====== Metedata of the chunk======:\n",
      "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning\n",
      "\n",
      "and discriminator (same number of layers, generator 25\n",
      "percent of the discriminator’s hidden layer size, hidden\n",
      "layer intermediate size, and number of heads). We copied\n",
      "Electra’s NLP conﬁguration with two changes: increasing\n",
      "the number of layers to 30 and using Lamb optimizer.\n",
      "Again, we split the training into two phases: the ﬁrst for\n",
      "proteins /C20512 residues (400k steps at 9k global batch size),\n",
      "the second for proteins /C201024 (400k steps at 3.5k global\n",
      "batch size). While ProtTXL, ProtBert, ProtAlbert and\n",
      "ProtXLNet relied on pre-computed tensorﬂow records as\n",
      "input, Electra allowed to mask sequences on the ﬂy, allow-\n",
      "ing the model to see different masking patterns during\n",
      "each epoch.\n",
      "ProtT5. Unlike the previous LMs, T56 uses an encoder\n",
      "and decoder [10]. We trained two model sizes, one with 3B\n",
      "(T5-XL) and one with 11B parameters (T5-XXL). T5-XL was\n",
      "trained using 8-way model parallelism, while T5-XXL was\n",
      "trained using 32-way model parallelism. First, T5-XL and\n",
      "T5-XXL were trained on BFD for 1.2M and 920k steps\n",
      "respectively (ProtT5-XL-BFD, ProtT5-XXL-BFD). In a second\n",
      "step, ProtT5-XL-BFD and ProtT5-XXL-BFD were ﬁne-tuned\n",
      "on UniRef50 for 991k and 343k steps respectively (ProtT5-\n",
      "\n",
      "====== Metedata of the chunk======:\n",
      "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning\n",
      "\n",
      "and discriminator (same number of layers, generator 25\n",
      "percent of the discriminator’s hidden layer size, hidden\n",
      "layer intermediate size, and number of heads). We copied\n",
      "Electra’s NLP conﬁguration with two changes: increasing\n",
      "the number of layers to 30 and using Lamb optimizer.\n",
      "Again, we split the training into two phases: the ﬁrst for\n",
      "proteins /C20512 residues (400k steps at 9k global batch size),\n",
      "the second for proteins /C201024 (400k steps at 3.5k global\n",
      "batch size). While ProtTXL, ProtBert, ProtAlbert and\n",
      "ProtXLNet relied on pre-computed tensorﬂow records as\n",
      "input, Electra allowed to mask sequences on the ﬂy, allow-\n",
      "ing the model to see different masking patterns during\n",
      "each epoch.\n",
      "ProtT5. Unlike the previous LMs, T56 uses an encoder\n",
      "and decoder [10]. We trained two model sizes, one with 3B\n",
      "(T5-XL) and one with 11B parameters (T5-XXL). T5-XL was\n",
      "trained using 8-way model parallelism, while T5-XXL was\n",
      "trained using 32-way model parallelism. First, T5-XL and\n",
      "T5-XXL were trained on BFD for 1.2M and 920k steps\n",
      "respectively (ProtT5-XL-BFD, ProtT5-XXL-BFD). In a second\n",
      "step, ProtT5-XL-BFD and ProtT5-XXL-BFD were ﬁne-tuned\n",
      "on UniRef50 for 991k and 343k steps respectively (ProtT5-\n",
      "\n",
      "====== Metedata of the chunk======:\n",
      "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning\n",
      "\n",
      "the previous ELMo/LSTM-based solution (DeepSeqVec).\n",
      "Increasing the corpus for pre-training the pLMs 10-fold\n",
      "appeared inconsequential (Prot* versus Prot*-BFD in Fig. 7\n",
      "and SOM Table 9, available online). In contrast, ﬁne-tuning\n",
      "ProtT5 models already trained on BFD using UniRef50\n",
      "improved (Prot*/Prot*-BFD versus Prot*-U50 in Fig. 7 and\n",
      "SOM Table 9, available online). Although most embedding-\n",
      "based approaches were outperformed by the SOA using\n",
      "MSAs (DeepLoc), both best ProtT5 models outperformed\n",
      "DeepLoc without MSAs: Q10, Fig. 7 and SOM Table 9, avail-\n",
      "able online.\n",
      "Similar for Membrane/Other.Results for the classiﬁcation\n",
      "into membrane/other (Q2; SOM Table 9, available online),\n",
      "largely conﬁrmed those obtained for location (Q10) and sec-\n",
      "ondary structure (Q3/Q8): (1) ProtT5 pLMs ﬁne-tuned on\n",
      "\n",
      "====== Metedata of the chunk======:\n",
      "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning\n",
      "\n",
      "scale predictions become, for the ﬁrst time since 30 years,\n",
      "feasible on commodity hardware. For instance, the best-per-\n",
      "forming model ProtT5-XL-U50 can run on a Nvidia TitanV\n",
      "with 12GB vRAM. Nevertheless, given the pLMs described\n",
      "here and elsewhere [34], [52], [53], [56], [91], [92], [93], we\n",
      "might expect an upper limit for what pLMs can learn\n",
      "7. Throughout this work, we used evolutionary information (EI) as\n",
      "synonymous for using multiple sequence alignments (MSAs). Whether\n",
      "pLMs do not implicitly extract EI will have to be proven in separate\n",
      "publications.\n",
      "ELNAGGAR ET AL.: PROTTRANS: TOWARD UNDERSTANDING THE LANGUAGE OF LIFE THROUGH SELF-SUPERVISED LEARNING 7123\n",
      "\n",
      "====== Metedata of the chunk======:\n",
      "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning\n",
      "\n",
      "scale predictions become, for the ﬁrst time since 30 years,\n",
      "feasible on commodity hardware. For instance, the best-per-\n",
      "forming model ProtT5-XL-U50 can run on a Nvidia TitanV\n",
      "with 12GB vRAM. Nevertheless, given the pLMs described\n",
      "here and elsewhere [34], [52], [53], [56], [91], [92], [93], we\n",
      "might expect an upper limit for what pLMs can learn\n",
      "7. Throughout this work, we used evolutionary information (EI) as\n",
      "synonymous for using multiple sequence alignments (MSAs). Whether\n",
      "pLMs do not implicitly extract EI will have to be proven in separate\n",
      "publications.\n",
      "ELNAGGAR ET AL.: PROTTRANS: TOWARD UNDERSTANDING THE LANGUAGE OF LIFE THROUGH SELF-SUPERVISED LEARNING 7123\n",
      "\n",
      "====== Metedata of the chunk======:\n",
      "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning\n",
      "\n",
      "scale predictions become, for the ﬁrst time since 30 years,\n",
      "feasible on commodity hardware. For instance, the best-per-\n",
      "forming model ProtT5-XL-U50 can run on a Nvidia TitanV\n",
      "with 12GB vRAM. Nevertheless, given the pLMs described\n",
      "here and elsewhere [34], [52], [53], [56], [91], [92], [93], we\n",
      "might expect an upper limit for what pLMs can learn\n",
      "7. Throughout this work, we used evolutionary information (EI) as\n",
      "synonymous for using multiple sequence alignments (MSAs). Whether\n",
      "pLMs do not implicitly extract EI will have to be proven in separate\n",
      "publications.\n",
      "ELNAGGAR ET AL.: PROTTRANS: TOWARD UNDERSTANDING THE LANGUAGE OF LIFE THROUGH SELF-SUPERVISED LEARNING 7123\n",
      "\n",
      "====== Metedata of the chunk======:\n",
      "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning\n",
      "\n",
      "performed theSOA without using MSAs (LA_ProtT5 & LA_ProtT5-U50\n",
      "[78]). The top row shows the complete range from 0-100, while the lower\n",
      "row zooms into the range of observed differences.\n",
      "ELNAGGAR ET AL.: PROTTRANS: TOWARD UNDERSTANDING THE LANGUAGE OF LIFE THROUGH SELF-SUPERVISED LEARNING 7121\n",
      "\n",
      "====== Metedata of the chunk======:\n",
      "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning\n",
      "\n",
      "performed theSOA without using MSAs (LA_ProtT5 & LA_ProtT5-U50\n",
      "[78]). The top row shows the complete range from 0-100, while the lower\n",
      "row zooms into the range of observed differences.\n",
      "ELNAGGAR ET AL.: PROTTRANS: TOWARD UNDERSTANDING THE LANGUAGE OF LIFE THROUGH SELF-SUPERVISED LEARNING 7121\n",
      "\n",
      "====== Metedata of the chunk======:\n",
      "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning\n",
      "\n",
      "performed theSOA without using MSAs (LA_ProtT5 & LA_ProtT5-U50\n",
      "[78]). The top row shows the complete range from 0-100, while the lower\n",
      "row zooms into the range of observed differences.\n",
      "ELNAGGAR ET AL.: PROTTRANS: TOWARD UNDERSTANDING THE LANGUAGE OF LIFE THROUGH SELF-SUPERVISED LEARNING 7121\n",
      "\n",
      "====== Metedata of the chunk======:\n",
      "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning\n",
      "\n",
      "TABLE 2\n",
      "Large-Scale Deep Learning:The Table Shows the Conﬁgurations for Pre-Training the Protein LMs Introduced Here (ProtTXL,\n",
      "ProtBert, ProtXLNet, ProtAlbert, ProtElectra, ProtT5) Using Either Summit, a TPU Pod v2 or a TPU Pod v3\n",
      "1. https://github.com/NVIDIA/DeepLearningExamples/\n",
      "2. https://github.com/google-research/bert\n",
      "3. https://github.com/google-research/albert\n",
      "ELNAGGAR ET AL.: PROTTRANS: TOWARD UNDERSTANDING THE LANGUAGE OF LIFE THROUGH SELF-SUPERVISED LEARNING 7115\n",
      "\n",
      "====== Metedata of the chunk======:\n",
      "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning\n",
      "\n",
      "TABLE 2\n",
      "Large-Scale Deep Learning:The Table Shows the Conﬁgurations for Pre-Training the Protein LMs Introduced Here (ProtTXL,\n",
      "ProtBert, ProtXLNet, ProtAlbert, ProtElectra, ProtT5) Using Either Summit, a TPU Pod v2 or a TPU Pod v3\n",
      "1. https://github.com/NVIDIA/DeepLearningExamples/\n",
      "2. https://github.com/google-research/bert\n",
      "3. https://github.com/google-research/albert\n",
      "ELNAGGAR ET AL.: PROTTRANS: TOWARD UNDERSTANDING THE LANGUAGE OF LIFE THROUGH SELF-SUPERVISED LEARNING 7115\n",
      "\n",
      "====== Metedata of the chunk======:\n",
      "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning\n",
      "\n",
      "TABLE 2\n",
      "Large-Scale Deep Learning:The Table Shows the Conﬁgurations for Pre-Training the Protein LMs Introduced Here (ProtTXL,\n",
      "ProtBert, ProtXLNet, ProtAlbert, ProtElectra, ProtT5) Using Either Summit, a TPU Pod v2 or a TPU Pod v3\n",
      "1. https://github.com/NVIDIA/DeepLearningExamples/\n",
      "2. https://github.com/google-research/bert\n",
      "3. https://github.com/google-research/albert\n",
      "ELNAGGAR ET AL.: PROTTRANS: TOWARD UNDERSTANDING THE LANGUAGE OF LIFE THROUGH SELF-SUPERVISED LEARNING 7115\n",
      "\n",
      "====== Metedata of the chunk======:\n",
      "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning\n",
      "\n",
      "the previous ELMo/LSTM-based solution (DeepSeqVec).\n",
      "Increasing the corpus for pre-training the pLMs 10-fold\n",
      "appeared inconsequential (Prot* versus Prot*-BFD in Fig. 7\n",
      "and SOM Table 9, available online). In contrast, ﬁne-tuning\n",
      "ProtT5 models already trained on BFD using UniRef50\n",
      "improved (Prot*/Prot*-BFD versus Prot*-U50 in Fig. 7 and\n",
      "SOM Table 9, available online). Although most embedding-\n",
      "based approaches were outperformed by the SOA using\n",
      "MSAs (DeepLoc), both best ProtT5 models outperformed\n",
      "DeepLoc without MSAs: Q10, Fig. 7 and SOM Table 9, avail-\n",
      "able online.\n",
      "Similar for Membrane/Other.Results for the classiﬁcation\n",
      "into membrane/other (Q2; SOM Table 9, available online),\n",
      "largely conﬁrmed those obtained for location (Q10) and sec-\n",
      "ondary structure (Q3/Q8): (1) ProtT5 pLMs ﬁne-tuned on\n",
      "UniRef50 performed best without MSAs, (2) the 10-fold\n",
      "larger pre-training BFD had no noticeable effect, (3) our best\n",
      "pLMs outperformed existing transformer pLMs (ESM-1b)\n",
      "(Fig. 7). In contrast to location and secondary structure, addi-\n",
      "tionally pre-training on UniRef50 appeared not to increase\n",
      "performance (SOM Table 9, available online) and both\n",
      "ProtT5 remained 1-2 percentage points below DeepLoc.\n",
      "\n",
      "====== Metedata of the chunk======:\n",
      "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning\n",
      "\n",
      "protein of both models for set NEW364 in more detail (SOM\n",
      "Fig. 12, available online), revealed that 57 percent of the\n",
      "ELNAGGAR ET AL.: PROTTRANS: TOWARD UNDERSTANDING THE LANGUAGE OF LIFE THROUGH SELF-SUPERVISED LEARNING 7119\n",
      "\n",
      "====== Metedata of the chunk======:\n",
      "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning\n",
      "\n",
      "also like to thank Konstantin Weißenow for helping with\n",
      "grant writing and providing early results for the structure\n",
      "prediction task. The authors would also like to thank both\n",
      "Adam Roberts and Colin Raffel for help with the T5 model,\n",
      "and the editor and the anonymous reviewers for essential\n",
      "criticism, especially, for suggesting to compare t-SNEs to\n",
      "randomly initialized models. The authors would also like to\n",
      "thank Leibniz Rechenzentrum (LRZ) for providing access\n",
      "7124 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 44, NO. 10, OCTOBER 2022\n",
      "\n",
      "====== Metedata of the chunk======:\n",
      "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning\n",
      "\n",
      "pLMs outperformed LocTree2 and a version of DeepLoc not using MSAs\n",
      "(DeepLoc-BLOSUM62). Only , ProtT5-XXL-U50 and ProtT5-XL-U50 out-\n",
      "performed theSOA. A recent method optimized location prediction from\n",
      "ProtT5 embeddings through a light-attention mechanism; it clearly out-\n",
      "performed theSOA without using MSAs (LA_ProtT5 & LA_ProtT5-U50\n",
      "[78]). The top row shows the complete range from 0-100, while the lower\n",
      "row zooms into the range of observed differences.\n",
      "ELNAGGAR ET AL.: PROTTRANS: TOWARD UNDERSTANDING THE LANGUAGE OF LIFE THROUGH SELF-SUPERVISED LEARNING 7121\n",
      "\n",
      "====== Metedata of the chunk======:\n",
      "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning\n",
      "\n",
      "contextualized word2vec-based approaches (DeepProtVec;\n",
      "Fig. 7, SOM Table 9, available online). Except for ProtTXL\n",
      "and ProtXLNet, all transformers trained here outperformed\n",
      "the previous ELMo/LSTM-based solution (DeepSeqVec).\n",
      "Increasing the corpus for pre-training the pLMs 10-fold\n",
      "appeared inconsequential (Prot* versus Prot*-BFD in Fig. 7\n",
      "and SOM Table 9, available online). In contrast, ﬁne-tuning\n",
      "ProtT5 models already trained on BFD using UniRef50\n",
      "improved (Prot*/Prot*-BFD versus Prot*-U50 in Fig. 7 and\n",
      "SOM Table 9, available online). Although most embedding-\n",
      "based approaches were outperformed by the SOA using\n",
      "MSAs (DeepLoc), both best ProtT5 models outperformed\n",
      "DeepLoc without MSAs: Q10, Fig. 7 and SOM Table 9, avail-\n",
      "able online.\n",
      "Similar for Membrane/Other.Results for the classiﬁcation\n",
      "into membrane/other (Q2; SOM Table 9, available online),\n",
      "largely conﬁrmed those obtained for location (Q10) and sec-\n",
      "ondary structure (Q3/Q8): (1) ProtT5 pLMs ﬁne-tuned on\n",
      "UniRef50 performed best without MSAs, (2) the 10-fold\n",
      "larger pre-training BFD had no noticeable effect, (3) our best\n",
      "pLMs outperformed existing transformer pLMs (ESM-1b)\n",
      "\n",
      "====== Metedata of the chunk======:\n",
      "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning\n",
      "\n",
      "contextualized word2vec-based approaches (DeepProtVec;\n",
      "Fig. 7, SOM Table 9, available online). Except for ProtTXL\n",
      "and ProtXLNet, all transformers trained here outperformed\n",
      "the previous ELMo/LSTM-based solution (DeepSeqVec).\n",
      "Increasing the corpus for pre-training the pLMs 10-fold\n",
      "appeared inconsequential (Prot* versus Prot*-BFD in Fig. 7\n",
      "and SOM Table 9, available online). In contrast, ﬁne-tuning\n",
      "ProtT5 models already trained on BFD using UniRef50\n",
      "improved (Prot*/Prot*-BFD versus Prot*-U50 in Fig. 7 and\n",
      "SOM Table 9, available online). Although most embedding-\n",
      "based approaches were outperformed by the SOA using\n",
      "MSAs (DeepLoc), both best ProtT5 models outperformed\n",
      "DeepLoc without MSAs: Q10, Fig. 7 and SOM Table 9, avail-\n",
      "able online.\n",
      "Similar for Membrane/Other.Results for the classiﬁcation\n",
      "into membrane/other (Q2; SOM Table 9, available online),\n",
      "largely conﬁrmed those obtained for location (Q10) and sec-\n",
      "ondary structure (Q3/Q8): (1) ProtT5 pLMs ﬁne-tuned on\n",
      "UniRef50 performed best without MSAs, (2) the 10-fold\n",
      "larger pre-training BFD had no noticeable effect, (3) our best\n",
      "pLMs outperformed existing transformer pLMs (ESM-1b)\n",
      "\n",
      "====== Metedata of the chunk======:\n",
      "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning\n",
      "\n",
      "contextualized word2vec-based approaches (DeepProtVec;\n",
      "Fig. 7, SOM Table 9, available online). Except for ProtTXL\n",
      "and ProtXLNet, all transformers trained here outperformed\n",
      "the previous ELMo/LSTM-based solution (DeepSeqVec).\n",
      "Increasing the corpus for pre-training the pLMs 10-fold\n",
      "appeared inconsequential (Prot* versus Prot*-BFD in Fig. 7\n",
      "and SOM Table 9, available online). In contrast, ﬁne-tuning\n",
      "ProtT5 models already trained on BFD using UniRef50\n",
      "improved (Prot*/Prot*-BFD versus Prot*-U50 in Fig. 7 and\n",
      "SOM Table 9, available online). Although most embedding-\n",
      "based approaches were outperformed by the SOA using\n",
      "MSAs (DeepLoc), both best ProtT5 models outperformed\n",
      "DeepLoc without MSAs: Q10, Fig. 7 and SOM Table 9, avail-\n",
      "able online.\n",
      "Similar for Membrane/Other.Results for the classiﬁcation\n",
      "into membrane/other (Q2; SOM Table 9, available online),\n",
      "largely conﬁrmed those obtained for location (Q10) and sec-\n",
      "ondary structure (Q3/Q8): (1) ProtT5 pLMs ﬁne-tuned on\n",
      "UniRef50 performed best without MSAs, (2) the 10-fold\n",
      "larger pre-training BFD had no noticeable effect, (3) our best\n",
      "pLMs outperformed existing transformer pLMs (ESM-1b)\n",
      "\n",
      "====== Metedata of the chunk======:\n",
      "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning\n",
      "\n",
      "ﬁrst training on BFD and then reﬁning on UniRef50. Consis-\n",
      "tently, all models ﬁne-tuned on UniRef50 outperformed\n",
      "those trained only on BFD (Fig. 4, SOM Tables 7, 6, available\n",
      "online). Although these gains were consistently numerically\n",
      "higher, the statistical signiﬁcance remained within the 68\n",
      "percent conﬁdence interval (maximal difference: 1.1 percent\n",
      "compared to one standard error of/C60.5%).\n",
      "Embeddings Reached State-of-the-Art (SOA). All models\n",
      "(ProtTXL, ProtBert, ProtAlbert, ProtXLNet, ProtElectra,\n",
      "ProtT5) and all databases (BFD, UniRef50/UniRef100) tested\n",
      "improved signiﬁcantly over context-free feature extractors\n",
      "such as word2vec-based approaches (DeepProtVec in Fig. 4\n",
      "and SOM Table 6, available online). Both ProtTXL versions\n",
      "fell short compared to an existing ELMo/LSTM-based solu-\n"
     ]
    }
   ],
   "source": [
    "retrived_docs = my_retriever.invoke(\"ProtTrans ProtElectra. Electra5 consists of two models\")\n",
    "print(f\"Retrieved {len(retrived_docs)} docs:\")\n",
    "for doc in retrived_docs:\n",
    "    print(f'\\n====== Metedata of the chunk======:\\n{doc.metadata['title']}\\n')\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337b68a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected m1 and m2 to have the same dtype, but got: float != double",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[413]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m query_embedding = embed_model.encode(\u001b[33m\"\u001b[39m\u001b[33mSaProt LLM model PLM SA-token structure-aware vocabulary\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Test similarity with your query\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m similarities = \u001b[43mcos_sim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m best_match_idx = similarities.argmax()\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBest match:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mall_embeddings[\u001b[33m'\u001b[39m\u001b[33mdocuments\u001b[39m\u001b[33m'\u001b[39m][best_match_idx][:\u001b[32m200\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.12/site-packages/sentence_transformers/util/similarity.py:45\u001b[39m, in \u001b[36mcos_sim\u001b[39m\u001b[34m(a, b)\u001b[39m\n\u001b[32m     43\u001b[39m a_norm = normalize_embeddings(a)\n\u001b[32m     44\u001b[39m b_norm = normalize_embeddings(b)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_norm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.to_dense()\n",
      "\u001b[31mRuntimeError\u001b[39m: expected m1 and m2 to have the same dtype, but got: float != double"
     ]
    }
   ],
   "source": [
    "from sentence_transformers.util import cos_sim\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#2. Check Vector Store Index\n",
    "#Verify documents were indexed correctly:\n",
    "\n",
    "# Get ALL document embeddings from the store\n",
    "all_embeddings = my_vectordb.get(include=['embeddings'])\n",
    "doc_embeddings = all_embeddings['embeddings']\n",
    "\n",
    "embed_model = SentenceTransformer(EMBED_MODEL_NAME, device=DEVICE, model_kwargs={'torch_dtype': torch.float})\n",
    "query_embedding = embed_model.encode(\"SaProt LLM model PLM SA-token structure-aware vocabulary\")\n",
    "\n",
    "# Test similarity with your query\n",
    "similarities = cos_sim(query_embedding, doc_embeddings)\n",
    "best_match_idx = similarities.argmax()\n",
    "print(f\"Best match:\\n{all_embeddings['documents'][best_match_idx][:200]}...\")\n",
    "\n",
    "# Print similarity score\n",
    "print(f\"Similarity: {similarities[0][best_match_idx].item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278e80ff",
   "metadata": {},
   "source": [
    "### New LangChain RAG retrieval implementation, the above code is deprecated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a30161",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'input'] optional_variables=['chat_history'] input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x11cc0ed40>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]} partial_variables={'chat_history': []} metadata={'lc_hub_owner': 'langchain-ai', 'lc_hub_repo': 'retrieval-qa-chat', 'lc_hub_commit_hash': 'b60afb6297176b022244feb83066e10ecadcda7b90423654c4a9d45e7a73cebc'} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='Answer any use questions based solely on the context below:\\n\\n<context>\\n{context}\\n</context>'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain import hub\n",
    "retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "print(retrieval_qa_chat_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "9523888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New LangChain implementation, the above code is deprecated.\n",
    "\n",
    "system_prompt = (\n",
    "    '''Use the given context (source documents and metadata) to answer the question. \n",
    "    If you don't know the answer, say you don't know. \n",
    "    Give the name of the source documents and metadata that supports your answer. \n",
    "    Context: {context}\"\n",
    "    '''\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(my_llm, prompt)\n",
    "chain = create_retrieval_chain(my_retriever, question_answer_chain)\n",
    "\n",
    "answer = chain.invoke({\"input\": '''\n",
    "             How does the SaProt protein-LLM differs from other protein-LLMs such as ProtTrans and ESM-2? \n",
    "             What are the key architectural novelties of this model?\n",
    "              '''})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bb4d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer['answer']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
